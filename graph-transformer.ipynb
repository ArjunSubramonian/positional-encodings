{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch_geometric.transforms as T\n",
    "from torch import lgamma\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_scatter import scatter_mean\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path \n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,2,3,6'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 0\n",
    "args.device = torch.device('cuda:'+ str(args.device) if torch.cuda.is_available() else 'cpu')\n",
    "# args.device = torch.device('cpu')\n",
    "print(\"device:\", args.device)\n",
    "# torch.cuda.set_device(args.device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, embed_dim, ff_embed_dim, num_heads, dropout, weights_dropout=True):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(layers):\n",
    "            self.layers.append(GraphTransformerLayer(embed_dim, ff_embed_dim, num_heads, dropout, weights_dropout))\n",
    "    \n",
    "    def forward(self, x, relation, kv = None,\n",
    "                self_padding_mask = None, self_attn_mask = None):\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x, _ = layer(x, relation, kv, self_padding_mask, self_attn_mask)\n",
    "        return x\n",
    "\n",
    "    def get_attn_weights(self, x, relation, kv = None,\n",
    "                self_padding_mask = None, self_attn_mask = None):\n",
    "        attns = []\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x, attn = layer(x, relation, kv, self_padding_mask, self_attn_mask, need_weights=True)\n",
    "            attns.append(attn)\n",
    "        attn = torch.stack(attns)\n",
    "        return attn\n",
    "\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, ff_embed_dim, num_heads, dropout, weights_dropout=True):\n",
    "        super(GraphTransformerLayer, self).__init__()\n",
    "        self.self_attn = RelationMultiheadAttention(embed_dim, num_heads, dropout, weights_dropout)\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_embed_dim)\n",
    "        self.fc2 = nn.Linear(ff_embed_dim, embed_dim)\n",
    "        self.attn_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = dropout\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.fc1.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc2.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc1.bias, 0.)\n",
    "        nn.init.constant_(self.fc2.bias, 0.)\n",
    "\n",
    "    def forward(self, x, relation, kv = None,\n",
    "                self_padding_mask = None, self_attn_mask = None,\n",
    "                need_weights = False):\n",
    "        # x: seq_len x bsz x embed_dim\n",
    "        residual = x\n",
    "        if kv is None:\n",
    "            x, self_attn = self.self_attn(query=x, key=x, value=x, relation=relation, key_padding_mask=self_padding_mask, attn_mask=self_attn_mask, need_weights=need_weights)\n",
    "        else:\n",
    "            x, self_attn = self.self_attn(query=x, key=kv, value=kv, relation=relation, key_padding_mask=self_padding_mask, attn_mask=self_attn_mask, need_weights=need_weights)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.attn_layer_norm(residual + x)\n",
    "\n",
    "        residual = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.ff_layer_norm(residual + x)\n",
    "        return x, self_attn\n",
    "\n",
    "class RelationMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., weights_dropout=True):\n",
    "        super(RelationMultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
    "        self.in_proj_bias = Parameter(torch.Tensor(3 * embed_dim))\n",
    "        self.relation_in_proj = nn.Linear(embed_dim, 2*embed_dim, bias=False)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.weights_dropout = weights_dropout\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.in_proj_weight, std=0.02)\n",
    "        nn.init.normal_(self.out_proj.weight, std=0.02)\n",
    "        nn.init.normal_(self.relation_in_proj.weight, std=0.02)\n",
    "        nn.init.constant_(self.in_proj_bias, 0.)\n",
    "        nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self, query, key, value, relation, key_padding_mask=None, attn_mask=None, need_weights=False):\n",
    "        \"\"\" Input shape: Time x Batch x Channel\n",
    "            relation:  tgt_len x src_len x bsz x dim\n",
    "            key_padding_mask: Time x batch\n",
    "            attn_mask:  tgt_len x src_len\n",
    "        \"\"\"\n",
    "        qkv_same = query.data_ptr() == key.data_ptr() == value.data_ptr()\n",
    "        kv_same = key.data_ptr() == value.data_ptr()\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        src_len = key.size(0)\n",
    "        assert key.size() == value.size()\n",
    "\n",
    "        if qkv_same:\n",
    "            # self-attention\n",
    "            q, k, v = self.in_proj_qkv(query)\n",
    "        elif kv_same:\n",
    "            # encoder-decoder attention\n",
    "            q = self.in_proj_q(query)\n",
    "            k, v = self.in_proj_kv(key)\n",
    "        else:\n",
    "            q = self.in_proj_q(query)\n",
    "            k = self.in_proj_k(key)\n",
    "            v = self.in_proj_v(value)\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim)\n",
    "        k = k.contiguous().view(src_len, bsz * self.num_heads, self.head_dim)\n",
    "        v = v.contiguous().view(src_len, bsz * self.num_heads, self.head_dim)\n",
    "\n",
    "        ra, rb = self.relation_in_proj(relation).chunk(2, dim=-1)\n",
    "        ra = ra.contiguous().view(tgt_len, src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        rb = rb.contiguous().view(tgt_len, src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        q = q.unsqueeze(1) + ra\n",
    "        k = k.unsqueeze(0) + rb\n",
    "        q *= self.scaling\n",
    "        # q: tgt_len x src_len x bsz*heads x dim\n",
    "        # k: tgt_len x src_len x bsz*heads x dim\n",
    "        # v: src_len x bsz*heads x dim\n",
    "\n",
    "        attn_weights = torch.einsum('ijbn,ijbn->ijb', [q, k])\n",
    "        assert list(attn_weights.size()) == [tgt_len, src_len, bsz * self.num_heads]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_weights.masked_fill_(\n",
    "                attn_mask.unsqueeze(-1),\n",
    "                float('-inf')\n",
    "            )\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # don't attend to padding symbols\n",
    "            attn_weights = attn_weights.view(tgt_len, src_len, bsz, self.num_heads)\n",
    "            attn_weights.masked_fill_(\n",
    "                key_padding_mask.unsqueeze(0).unsqueeze(-1),\n",
    "                float('-inf')\n",
    "            )\n",
    "            attn_weights = attn_weights.view(tgt_len, src_len, bsz * self.num_heads)\n",
    "\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "\n",
    "        if self.weights_dropout:\n",
    "            attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        # attn_weights: tgt_len x src_len x bsz*heads\n",
    "        # v: src_len x bsz*heads x dim\n",
    "        attn = torch.einsum('ijb,jbn->bin', [attn_weights, v])\n",
    "        if not self.weights_dropout:\n",
    "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "\n",
    "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "\n",
    "        if need_weights:\n",
    "            # maximum attention weight over heads \n",
    "            attn_weights = attn_weights.view(tgt_len, src_len, bsz, self.num_heads)\n",
    "        else:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn, attn_weights\n",
    "\n",
    "    def in_proj_qkv(self, query):\n",
    "        return self._in_proj(query).chunk(3, dim=-1)\n",
    "\n",
    "    def in_proj_kv(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim).chunk(2, dim=-1)\n",
    "\n",
    "    def in_proj_q(self, query):\n",
    "        return self._in_proj(query, end=self.embed_dim)\n",
    "\n",
    "    def in_proj_k(self, key):\n",
    "        return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
    "\n",
    "    def in_proj_v(self, value):\n",
    "        return self._in_proj(value, start=2 * self.embed_dim)\n",
    "\n",
    "    def _in_proj(self, input, start=0, end=None):\n",
    "        weight = self.in_proj_weight\n",
    "        bias = self.in_proj_bias\n",
    "        weight = weight[start:end, :]\n",
    "        if bias is not None:\n",
    "            bias = bias[start:end]\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_shortest_distances(d):\n",
    "    d_nx = to_networkx(d, to_undirected=True)\n",
    "    p = shortest_path(d_nx)\n",
    "    \n",
    "    sd_edge_index = torch.LongTensor(2, d.x.size(0) * d.x.size(0))\n",
    "    sd_edge_attr = torch.FloatTensor(d.x.size(0) * d.x.size(0), 1)\n",
    "    for i in range(d.x.size(0)):\n",
    "        for j in range(d.x.size(0)):\n",
    "            sd_edge_index[0][i * d.x.size(0) + j] = i\n",
    "            sd_edge_index[1][i * d.x.size(0) + j] = j\n",
    "            \n",
    "            if j in p[i]:\n",
    "                sd_edge_attr[i * d.x.size(0) + j] = len(p[i][j]) - 1\n",
    "            else:\n",
    "                sd_edge_attr[i * d.x.size(0) + j] = float(\"inf\")\n",
    "        \n",
    "    return Data(x=d.x, y=d.y, edge_index=d.edge_index, edge_attr=d.edge_attr, sd_edge_index=sd_edge_index, sd_edge_attr=sd_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embed_dim // num_heads should remain constant\n",
    "args.dataset = 'ogbg-moltox21'\n",
    "args.n_classes = 12\n",
    "args.batch_size = 8\n",
    "args.lr = 0.001\n",
    "args.graph_pooling = 'mean'\n",
    "args.proj_mode = 'nonlinear'\n",
    "args.eval_metric = 'rocauc'\n",
    "args.embed_dim = 512\n",
    "args.ff_embed_dim = 1024\n",
    "args.num_heads = 8\n",
    "args.graph_layers = 4\n",
    "args.dropout = 0.2\n",
    "args.relation_type = 'shortest_dist'\n",
    "args.pre_transform = compute_mutual_shortest_distances\n",
    "args.max_vocab = 12\n",
    "args.split = 'scaffold'\n",
    "args.num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, nclasses, layers, embed_dim, ff_embed_dim, num_heads, dropout, relation_type, max_vocab, weights_dropout=True):\n",
    "        super(GraphTransformerModel, self).__init__()\n",
    "        self.model_type = 'GraphTransformerModel'\n",
    "        self.encoder = AtomEncoder(emb_dim=embed_dim)\n",
    "        self.transformer = GraphTransformer(layers, embed_dim, ff_embed_dim, num_heads, dropout, weights_dropout)\n",
    "        \n",
    "        #Different kind of graph pooling\n",
    "        if args.graph_pooling == \"sum\":\n",
    "            self.graph_pool = global_add_pool\n",
    "        elif args.graph_pooling == \"mean\":\n",
    "            self.graph_pool = global_mean_pool\n",
    "        elif args.graph_pooling == \"max\":\n",
    "            self.graph_pool = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "        \n",
    "        self.task_pred = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, nclasses)\n",
    "        )\n",
    "        \n",
    "        self.relation_type = relation_type\n",
    "        self.max_vocab = max_vocab\n",
    "        self.relation_encoder = nn.Embedding(max_vocab, embed_dim)\n",
    "        \n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        x, mask = to_dense_batch(self.encoder(src.x), batch=src.batch, fill_value=0)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        if self.relation_type == 'link':\n",
    "            relation = self.relation_encoder(to_dense_adj(src.edge_index, batch=src.batch, max_num_nodes=x.size(0)).long())\n",
    "        elif self.relation_type == 'shortest_dist':\n",
    "            mod_sd_edge_attr = torch.clamp(src.sd_edge_attr.reshape(-1), 0, self.max_vocab - 1).long()\n",
    "            relation = self.relation_encoder(to_dense_adj(src.sd_edge_index, batch=src.batch, edge_attr=mod_sd_edge_attr, max_num_nodes=x.size(0)).long())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid relation type.\")\n",
    "        \n",
    "        relation = relation.permute(2, 1, 0, 3)\n",
    "        \n",
    "        output = self.transformer(x, relation, self_padding_mask=~mask.transpose(0, 1))\n",
    "        \n",
    "        graph_emb = self.graph_pool(output.transpose(0, 1)[mask], src.batch)\n",
    "        graph_pred = self.task_pred(graph_emb)\n",
    "        \n",
    "        y = batch.y.float()\n",
    "        is_valid = ~torch.isnan(y)\n",
    "        loss = self.criterion(graph_pred[is_valid], y[is_valid])\n",
    "        \n",
    "        return torch.unsqueeze(loss,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-moltox21 \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=args.pre_transform).shuffle()\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "if args.split == 'scaffold':\n",
    "    train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "elif args.split == '80-20':\n",
    "    train_loader = DataLoader(dataset[:int(0.8 * len(dataset))], batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "    test_loader = DataLoader(dataset[int(0.8 * len(dataset)):], batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "model = GraphTransformerModel(args.n_classes, args.graph_layers, args.embed_dim, args.ff_embed_dim, args.num_heads, args.dropout, args.relation_type, args.max_vocab)\n",
    "model = nn.DataParallel(model).to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        loss = model(batch.to(args.device))\n",
    "        loss = loss.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    print('Train loss:', loss_epoch / len(train_loader))\n",
    "    \n",
    "    ##############\n",
    "    # EVALUATION #\n",
    "    ##############\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            loss = model(batch.to(args.device))\n",
    "            loss = loss.sum()\n",
    "\n",
    "            # y = batch.y.float()\n",
    "            # y_true.append(y)\n",
    "            # y_scores.append(z)\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "        \n",
    "        # y_true = torch.cat(y_true, dim = 0)\n",
    "        # y_scores = torch.cat(y_scores, dim = 0)\n",
    "\n",
    "    # input_dict = {\"y_true\": y_true, \"y_pred\": y_scores}\n",
    "    # result_dict = evaluator.eval(input_dict)\n",
    "    print('Test loss:', loss_epoch / len(test_loader))\n",
    "    # print('Test ROC-AUC:', result_dict[args.eval_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODOs:\n",
    "# ogbg-molhiv, molpcba, try all OGB leaderboard datasets\n",
    "# explore network feature extraction using Networkx, survey (pick 5-10)\n",
    "# try features independently, then try different compositions -- analyze thoroughly\n",
    "# continuous relation features: https://arxiv.org/abs/2003.09229\n",
    "# link prediction is the simplest structural task to implement\n",
    "# TransformerXL has different way to do relative positional encoding\n",
    "# https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py#L92\n",
    "# Blockwise Self-Attention for Long Document Understanding\n",
    "# Longformer <-- standard way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
