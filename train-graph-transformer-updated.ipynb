{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import DataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from graph_transformer import GT\n",
    "from utils import pre_process, get_n_params\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.dataset = 'ogbg-molhiv'\n",
    "args.n_classes = 1\n",
    "args.lr = 2e-4\n",
    "args.n_hid = 512\n",
    "args.n_heads = 8\n",
    "args.n_layer = 4\n",
    "args.dropout = 0.2\n",
    "args.num_epochs = 100\n",
    "args.k_hop_neighbors = 3\n",
    "args.weight_decay = 1e-2\n",
    "args.bsz      = 128\n",
    "args.strategies = ['ea', 'sd', 'cn']\n",
    "args.summary_node = True\n",
    "args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-molhiv \n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7487/41127 [00:00<00:00, 74865.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 70922.19it/s]\n",
      " 16%|█▌        | 6575/41127 [00:00<00:00, 37297.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 70677.66it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=lambda d : pre_process(d, args), root = f'dataset/{args.k_hop_neighbors}')\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "split_idx = dataset.get_idx_split()\n",
    "edge_dim_dict = {'ea': dataset.data.edge_attr.max().int().item() + 1, \\\n",
    "                 'sd': dataset.data.sd_edge_attr.max().int().item() + 1, \\\n",
    "                 'cn': dataset.data.cn_edge_attr.max().int().item() + 1}\n",
    "model = GT(args.n_hid, args.n_classes, args.n_heads, args.n_layer, edge_dim_dict, args.dropout, args.summary_node).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def get_optimizer(model: nn.Module, learning_rate: float = 1e-4, adam_eps: float = 1e-6,\n",
    "                  weight_decay: float = 0.0) -> torch.optim.Optimizer:\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_eps)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model #Params: %d' % get_n_params(model))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay = args.weight_decay)\n",
    "\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.bsz, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle=False)\n",
    "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, pct_start = 0.05,\\\n",
    "        steps_per_epoch=len(train_loader), epochs = args.num_epochs, anneal_strategy = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for num_iters, data in enumerate(tqdm(train_loader)):\n",
    "        data.to(args.device)\n",
    "        out = model(data.x, data.batch, data.edge_index, {'ea': data.edge_attr, 'cn': data.cn_edge_attr, 'sd': data.sd_edge_attr})\n",
    "        loss = criterion(out, data.y.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += [loss.item()]\n",
    "        \n",
    "    args.writer.add_scalar(\"LR/epoch\", optimizer.param_groups[0]['lr'], epoch + 1)\n",
    "    args.writer.add_scalar(\"Loss/train\", np.average(train_loss), epoch + 1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for num_iters, data in enumerate(tqdm(valid_loader)):\n",
    "            data.to(args.device)\n",
    "            out = model(data.x, data.batch, data.edge_index, {'ea': data.edge_attr, 'cn': data.cn_edge_attr, 'sd': data.sd_edge_attr})\n",
    "\n",
    "            loss = criterion(out, data.y.float())\n",
    "            valid_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        valid_rocauc = evaluator.eval(input_dict)['rocauc']\n",
    "        args.writer.add_scalar(\"Loss/valid\", np.average(valid_loss), epoch + 1)\n",
    "        args.writer.add_scalar(\"ROC/valid\", valid_rocauc, epoch + 1)\n",
    "        \n",
    "        test_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for data in test_loader:\n",
    "            data.to(args.device)\n",
    "            out = model(data.x, data.batch, data.edge_index, {'ea': data.edge_attr, 'cn': data.cn_edge_attr, 'sd': data.sd_edge_attr})\n",
    "\n",
    "            loss = criterion(out, data.y.float())\n",
    "            test_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        test_rocauc = evaluator.eval(input_dict)['rocauc']\n",
    "        args.writer.add_scalar(\"Loss/test\", np.average(test_loss), epoch + 1)\n",
    "        args.writer.add_scalar(\"ROC/test\", test_rocauc, epoch + 1)\n",
    "    \n",
    "    print('Epoch %d: LR: %.5f, Train loss: %.3f Valid loss: %.3f  Valid ROC-AUC: %.3f Test loss: %.3f  Test ROC-AUC: %.3f' \\\n",
    "          % (epoch + 1, optimizer.param_groups[0]['lr'], np.average(train_loss), np.average(valid_loss), \\\n",
    "            valid_rocauc, np.average(test_loss), test_rocauc))\n",
    "    stats += [[epoch, np.average(train_loss), np.average(valid_loss), valid_rocauc, np.average(test_loss), test_rocauc]]\n",
    "\n",
    "args.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
