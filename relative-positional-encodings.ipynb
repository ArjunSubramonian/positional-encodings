{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch_geometric.transforms as T\n",
    "from torch import lgamma\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_scatter import scatter_mean\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 3\n",
    "args.device = torch.device('cuda:'+ str(args.device) if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", args.device)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention_RPR(nn.Module):\n",
    "    def __init__(self, d_model, h, max_relative_position, dropout=.0):\n",
    "        \"\"\"\n",
    "        multi-head attention\n",
    "        :param h: nhead\n",
    "        :param d_model: d_model\n",
    "        :param dropout: float\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention_RPR, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        #  assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = utils.clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.vocab_size = max_relative_position * 2 + 1\n",
    "        self.embed_K = nn.Embedding(self.vocab_size, self.d_k)\n",
    "        self.embed_V = nn.Embedding(self.vocab_size, self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        ---------------------------\n",
    "        L : target sequence length\n",
    "        S : source sequence length:\n",
    "        N : batch size\n",
    "        E : embedding dim\n",
    "        ---------------------------\n",
    "        :param query: (N,L,E)\n",
    "        :param key: (N,S,E)\n",
    "        :param value: (N,S,E)\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "        nbatches = query.size(0)  # batch size\n",
    "        seq_len = query.size(1)\n",
    "        # 1) split embedding dim to h heads : from d_model => h * d_k\n",
    "        # dim: (nbatch, h, seq_length, d_model//h)\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) rpr\n",
    "        relation_keys = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_K)\n",
    "        relation_values = self.generate_relative_positions_embeddings(seq_len, seq_len, self.embed_V)\n",
    "        logits = self._relative_attn_inner(query, key, relation_keys, True)\n",
    "        weights = self.dropout(F.softmax(logits, -1))\n",
    "        x = self._relative_attn_inner(weights, value, relation_values, False)\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # dim: (nbatch, h, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def _generate_relative_positions_matrix(self, len_q, len_k):\n",
    "        \"\"\"\n",
    "        genetate rpr matrix\n",
    "        ---------------------------\n",
    "        :param len_q: seq_len\n",
    "        :param len_k: seq_len\n",
    "        :return: rpr matrix, dim: (len_q, len_q)\n",
    "        \"\"\"\n",
    "        assert len_q == len_k\n",
    "        range_vec_q = range_vec_k = torch.arange(len_q)\n",
    "        distance_mat = range_vec_k.unsqueeze(0) - range_vec_q.unsqueeze(-1)\n",
    "        disntance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        return disntance_mat_clipped + self.max_relative_position\n",
    "\n",
    "    def generate_relative_positions_embeddings(self, len_q, len_k, embedding_table):\n",
    "        \"\"\"\n",
    "        generate relative position embedding\n",
    "        ----------------------\n",
    "        :param len_q:\n",
    "        :param len_k:\n",
    "        :return: rpr embedding, dim: (len_q, len_q, d_k)\n",
    "        \"\"\"\n",
    "        relative_position_matrix = self._generate_relative_positions_matrix(len_q, len_k)\n",
    "        return embedding_table(relative_position_matrix)\n",
    "\n",
    "    def _relative_attn_inner(self, x, y, z, transpose):\n",
    "        \"\"\"\n",
    "        efficient implementation\n",
    "        ------------------------\n",
    "        :param x: \n",
    "        :param y: \n",
    "        :param z: \n",
    "        :param transpose: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        nbatches = x.size(0)\n",
    "        heads = x.size(1)\n",
    "        seq_len = x.size(2)\n",
    "\n",
    "        # (N, h, s, s)\n",
    "        xy_matmul = torch.matmul(x, y.transpose(-1, -2) if transpose else y)\n",
    "        # (s, N, h, d) => (s, N*h, d)\n",
    "        x_t_v = x.permute(2, 0, 1, 3).contiguous().view(seq_len, nbatches * heads, -1)\n",
    "        # (s, N*h, d) @ (s, d, s) => (s, N*h, s)\n",
    "        x_tz_matmul = torch.matmul(x_t_v, z.transpose(-1, -2) if transpose else z)\n",
    "        # (N, h, s, s)\n",
    "        x_tz_matmul_v_t = x_tz_matmul.view(seq_len, nbatches, heads, -1).permute(1, 2, 0, 3)\n",
    "        return xy_matmul + x_tz_matmul_v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention_RPR(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "[docs]    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder\n",
    "        self.model_type = 'Transformer'\n",
    "        # self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
