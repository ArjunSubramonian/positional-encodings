{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from graph_transformer import GT\n",
    "from utils import pre_process, pre_process_with_summary, concat_pre_process_with_summary, inf_sum_pre_process_with_summary, fin_sum_pre_process_with_summary, get_n_params, get_optimizer\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.dataset = 'ogbg-molhiv'\n",
    "args.n_classes = 1\n",
    "args.lr = 3e-4\n",
    "args.n_hid = 512\n",
    "args.n_heads = 8\n",
    "args.n_layer = 4\n",
    "args.dropout = 0.3\n",
    "args.num_epochs = 50\n",
    "args.k_hop_neighbors = 3\n",
    "args.k_hop = True\n",
    "args.weight_decay = 1e-2\n",
    "# args.bsz      = 512\n",
    "args.bsz      = 128\n",
    "args.strategies = ['ea', 'rw_inf_sum']\n",
    "args.summary_node = True\n",
    "args.hier_levels = 3\n",
    "args.lap_k = None\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.metric = 'rocauc'\n",
    "print(\"device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-molhiv \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "tz = pytz.timezone('US/Pacific')\n",
    "time_now = datetime.datetime.now(tz).strftime('%m-%d_%H:%M:%S')\n",
    "\n",
    "if args.summary_node:\n",
    "    pre_transform = lambda d : inf_sum_pre_process_with_summary(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/inf_sum_with_summary_{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/inf_sum_with_summary_{args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "\n",
    "else:\n",
    "    pre_transform = lambda d : pre_process(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "    \n",
    "    \n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=pre_transform, root = root_path)\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "split_idx = dataset.get_idx_split()\n",
    "edge_dim_dict = {'ea': None, \\\n",
    "                 'disc': {\n",
    "#                      'sd': (dataset.data.sd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'cn': (dataset.data.cn_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'hsd': (dataset.data.hsd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "                    },\n",
    "                 'cont': {\n",
    "#                      **{('rw_' + str(k)): args.n_hid for k in range(1, args.k_hop_neighbors + 1)}\n",
    "                     'rw': args.n_hid\n",
    "                 }\n",
    "                }\n",
    "model = GT(args.n_hid, args.n_classes, args.n_heads, args.n_layer, edge_dim_dict, args.dropout, args.summary_node, args.lap_k).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 7717377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Model #Params: %d' % get_n_params(model))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay = args.weight_decay, learning_rate = args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "scheduler.step(-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "def mat_visualize(node_size, edge_index, edge_attr):\n",
    "    mat = np.zeros((node_size, node_size))\n",
    "    for e, v in zip(edge_index, edge_attr):\n",
    "        mat[e[0]][e[1]] = v\n",
    "    sb.heatmap(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_prob(inp):\n",
    "    prob = torch.sigmoid(inp)\n",
    "    prob = torch.cat([prob, 1-prob], dim=1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "valid_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "test_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "\n",
    "train_mask[split_idx[\"train\"]] = True\n",
    "valid_mask[split_idx[\"valid\"]] = True\n",
    "test_mask[split_idx[\"test\"]] = True\n",
    "def entropy_loss(pred, label):\n",
    "    return torch.mean(torch.sum(-label * pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:10<00:00,  1.68it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  4.74it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: LR: 0.00022, Train loss: 0.182 Train rocauc: 0.519 Train Adv: 0.205 Valid loss: 0.093  Valid rocauc: 0.700         Test loss: 0.138  Test rocauc: 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:08<00:00,  1.71it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  5.34it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: LR: 0.00024, Train loss: 0.157 Train rocauc: 0.610 Train Adv: 0.154 Valid loss: 0.091  Valid rocauc: 0.710         Test loss: 0.144  Test rocauc: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:09<00:00,  1.70it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  5.16it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: LR: 0.00001, Train loss: 0.148 Train rocauc: 0.690 Train Adv: 0.149 Valid loss: 0.086  Valid rocauc: 0.738         Test loss: 0.122  Test rocauc: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:09<00:00,  1.69it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  4.99it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: LR: 0.00018, Train loss: 0.142 Train rocauc: 0.719 Train Adv: 0.146 Valid loss: 0.088  Valid rocauc: 0.738         Test loss: 0.121  Test rocauc: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:09<00:00,  1.69it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  5.15it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: LR: 0.00027, Train loss: 0.147 Train rocauc: 0.681 Train Adv: 0.146 Valid loss: 0.099  Valid rocauc: 0.672         Test loss: 0.129  Test rocauc: 0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:07<00:00,  1.71it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  5.13it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: LR: 0.00002, Train loss: 0.139 Train rocauc: 0.724 Train Adv: 0.144 Valid loss: 0.081  Valid rocauc: 0.766         Test loss: 0.123  Test rocauc: 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [03:08<00:00,  1.70it/s]\n",
      "100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n",
      "  0%|          | 0/321 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: LR: 0.00015, Train loss: 0.136 Train rocauc: 0.745 Train Adv: 0.140 Valid loss: 0.088  Valid rocauc: 0.744         Test loss: 0.119  Test rocauc: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 292/321 [02:52<00:17,  1.68it/s]"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_adv  = []\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    all_idx = torch.randperm(len(dataset))\n",
    "    for batch_idx in tqdm(range(len(all_idx) // args.bsz)):\n",
    "        batch = all_idx[batch_idx * args.bsz : (batch_idx + 1) * args.bsz]\n",
    "        train_msk = train_mask[batch]    \n",
    "        data = Batch.from_data_list(dataset[batch])\n",
    "        data.to(args.device)\n",
    "        \n",
    "#         strats = {'ea': data.edge_attr, \\\n",
    "#                   **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "        strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "        out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        with torch.no_grad():\n",
    "#             strats = {'ea': data.edge_attr, \\\n",
    "#                   **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            adv_out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "        loss = criterion(out[train_msk], data.y[train_msk].float())\n",
    "        adv_loss = entropy_loss(turn_prob(out).log(), turn_prob(adv_out))\n",
    "        (loss + 0.5 * adv_loss).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += [loss.item()]\n",
    "        train_adv  += [adv_loss.item()]\n",
    "        \n",
    "        y_true += [data.y]\n",
    "        y_scores += [out]\n",
    "\n",
    "    input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "    train_metric = evaluator.eval(input_dict)[args.metric]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for num_iters, data in enumerate(tqdm(valid_loader)):\n",
    "            data.to(args.device)\n",
    "#             strats = {'ea': data.edge_attr, \\\n",
    "#                   **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            loss = criterion(out, data.y.float())\n",
    "            valid_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        valid_metric = evaluator.eval(input_dict)[args.metric]\n",
    "        \n",
    "        test_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for data in test_loader:\n",
    "            data.to(args.device)\n",
    "#             strats = {'ea': data.edge_attr, \\\n",
    "#                   **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            loss = criterion(out, data.y.float())\n",
    "            test_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        test_metric = evaluator.eval(input_dict)[args.metric]\n",
    "\n",
    "    print('Epoch %d: LR: %.5f, Train loss: %.3f Train %s: %.3f Train Adv: %.3f Valid loss: %.3f  Valid %s: %.3f \\\n",
    "        Test loss: %.3f  Test %s: %.3f' \\\n",
    "          % (epoch + 1, optimizer.param_groups[0]['lr'], np.average(train_loss), args.metric, train_metric, \\\n",
    "             np.average(train_adv), np.average(valid_loss), args.metric, valid_metric, \\\n",
    "             np.average(test_loss), args.metric, test_metric))\n",
    "    stats += [[epoch, np.average(train_loss), train_metric, np.average(valid_loss), valid_metric, np.average(test_loss), test_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = ['epoch', 'train_loss', 'train_metric', 'valid_loss', 'valid_metric', 'test_loss', 'test_metric']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "stats_np = np.array(stats)\n",
    "best_valid = stats_np[stats_np[:50, 4].argmax()]\n",
    "print(best_valid)\n",
    "for i in range(1, stats_np.shape[-1]):\n",
    "    ax = fig.add_subplot(2, 3, i)\n",
    "    ax.plot(stats_np[:, i], label=labels[i])\n",
    "    ax.scatter(x=best_valid[0], y=best_valid[i], color='red')\n",
    "    ax.annotate(best_valid[i].round(3), xy=(best_valid[0]+5, best_valid[i]), color='red')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, to_networkx, dense_to_sparse, remove_self_loops, to_undirected\n",
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "\n",
    "model.eval()\n",
    "data = dataset[3]    \n",
    "data.to(args.device)\n",
    "strats = {'ea': data.edge_attr,  'rw': data.rw_edge_attr}\n",
    "out = model(data.x, 0, data.edge_index, strats)\n",
    "\n",
    "threshold = 0.975\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=len(model.gcs), ncols=2, figsize=(15, 10 * len(model.gcs)))\n",
    "for layer_idx, gc in enumerate(model.gcs):\n",
    "    imgs = []\n",
    "    \n",
    "    adj = to_dense_adj(edge_index=data.edge_index, edge_attr=gc.att)[0]\n",
    "    adj_mean = adj.mean(dim=-1).detach().cpu()\n",
    "    adj_max = adj.max(dim=-1)[0].detach().cpu()\n",
    "    \n",
    "    adj_mean_sorted = adj_mean.flatten().sort()[0]\n",
    "    adj_mean_threshold = adj_mean_sorted[int(threshold * len(adj_mean_sorted))]\n",
    "    adj_mean[adj_mean < adj_mean_threshold] = 0\n",
    "    adj_mean[adj_mean >= adj_mean_threshold] = 1\n",
    "    \n",
    "#     adj_max_sorted = adj_max.flatten().sort()[0]\n",
    "#     adj_max_threshold = adj_max_sorted[int(threshold * len(adj_max_sorted))]\n",
    "#     adj_max[adj_max < max_threshold] = 0\n",
    "#     adj_max[adj_max >= max_threshold] = 1\n",
    "    \n",
    "    mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "    mean_edge_index = remove_self_loops(mean_edge_index)[0]\n",
    "    mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "    \n",
    "#     max_edge_index = dense_to_sparse(adj_max.long())[0]\n",
    "#     max_edge_index = remove_self_loops(max_edge_index)[0]\n",
    "#     max_data = Data(x=data.x, edge_index=max_edge_index)\n",
    "    \n",
    "    # ax = axes[layer_idx][0]\n",
    "    # ax.set_title(f'Layer {layer_idx + 1}, Mean')\n",
    "    # im = ax.matshow(adj_mean)\n",
    "    # fig.colorbar(im, ax=ax)\n",
    "    # molecule_draw_with_color(to_networkx(mean_data, node_attrs=['x']), ax=ax, labels='node_id')\n",
    "    mol = pyg_to_mol(mean_data)  \n",
    "    # mc = mol_to_svg(mol, molSize=(150, 150))\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    \n",
    "    # ax = axes[layer_idx][1]\n",
    "    # ax.set_title(f'Layer {layer_idx + 1}, Max')\n",
    "    # im = ax.matshow(adj_max)\n",
    "    # fig.colorbar(im, ax=ax)\n",
    "    # molecule_draw_with_color(to_networkx(max_data, node_attrs=['x']), ax=ax, labels='node_id')\n",
    "    \n",
    "    # mol = pyg_to_mol(max_data)  \n",
    "    # mc = mol_to_svg(mol, molSize=(150, 150))\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    # imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
