{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from graph_transformer import GT\n",
    "from utils import pre_process, pre_process_with_summary, basic_pre_process_with_summary, get_n_params, get_optimizer\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.dataset = 'ogbg-molhiv'\n",
    "args.n_classes = 1\n",
    "args.lr = 3e-4\n",
    "args.n_hid = 512\n",
    "args.n_heads = 8\n",
    "args.n_layer = 4\n",
    "args.dropout = 0.3\n",
    "args.num_epochs = 50\n",
    "args.k_hop_neighbors = 3\n",
    "args.weight_decay = 1e-2\n",
    "args.bsz      = 512\n",
    "args.strategies = ['ea', 'rw']\n",
    "args.summary_node = True\n",
    "args.hier_levels = 3\n",
    "args.lap_k = None\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.metric = 'rocauc'\n",
    "print(\"device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-molhiv \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:00<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/ogbg-molhiv/basic_with_summary_3/hiv.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7246/41127 [00:00<00:00, 72453.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 73623.38it/s]\n",
      "  0%|          | 0/41127 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 58925.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "tz = pytz.timezone('US/Pacific')\n",
    "time_now = datetime.datetime.now(tz).strftime('%m-%d_%H:%M:%S')\n",
    "\n",
    "if args.summary_node:\n",
    "    pre_transform = lambda d : basic_pre_process_with_summary(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/basic_with_summary_{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/with_summary_k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "\n",
    "else:\n",
    "    pre_transform = lambda d : pre_process(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "    \n",
    "    \n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=pre_transform, root = root_path)\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "split_idx = dataset.get_idx_split()\n",
    "edge_dim_dict = {'ea': None, \\\n",
    "                 'disc': {\n",
    "#                      'sd': (dataset.data.sd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'cn': (dataset.data.cn_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'hsd': (dataset.data.hsd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "                    },\n",
    "                 'cont': {\n",
    "                     'rw': args.n_hid\n",
    "                 }\n",
    "                }\n",
    "model = GT(args.n_hid, args.n_classes, args.n_heads, args.n_layer, edge_dim_dict, args.dropout, args.summary_node, args.lap_k).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    if d.edge_attr.size(0) != d.rw_edge_attr.size(0):\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 7717377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Model #Params: %d' % get_n_params(model))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay = args.weight_decay, learning_rate = args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "scheduler.step(-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "def mat_visualize(node_size, edge_index, edge_attr):\n",
    "    mat = np.zeros((node_size, node_size))\n",
    "    for e, v in zip(edge_index, edge_attr):\n",
    "        mat[e[0]][e[1]] = v\n",
    "    sb.heatmap(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_prob(inp):\n",
    "    prob = torch.sigmoid(inp)\n",
    "    prob = torch.cat([prob, 1-prob], dim=1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "valid_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "test_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "\n",
    "train_mask[split_idx[\"train\"]] = True\n",
    "valid_mask[split_idx[\"valid\"]] = True\n",
    "test_mask[split_idx[\"test\"]] = True\n",
    "def entropy_loss(pred, label):\n",
    "    return torch.mean(torch.sum(-label * pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:15<00:00,  1.05it/s]\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.28it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: LR: 0.00002, Train loss: 0.226 Train rocauc: 0.498 Train Adv: 0.328 Valid loss: 0.091  Valid rocauc: 0.409         Test loss: 0.138  Test rocauc: 0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:17<00:00,  1.04it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  3.13it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: LR: 0.00007, Train loss: 0.163 Train rocauc: 0.488 Train Adv: 0.163 Valid loss: 0.088  Valid rocauc: 0.669         Test loss: 0.129  Test rocauc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 16/80 [00:15<01:00,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_adv  = []\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    all_idx = torch.randperm(len(dataset))\n",
    "    for batch_idx in tqdm(range(len(all_idx) // args.bsz)):\n",
    "        batch = all_idx[batch_idx * args.bsz : (batch_idx + 1) * args.bsz]\n",
    "        train_msk = train_mask[batch]    \n",
    "        data = Batch.from_data_list(dataset[batch])\n",
    "        data.to(args.device)\n",
    "        \n",
    "        strats = {'ea': data.edge_attr,  'rw': data.rw_edge_attr}\n",
    "        out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        with torch.no_grad():\n",
    "            strats = {'ea': data.edge_attr,  'rw': data.rw_edge_attr}\n",
    "            adv_out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "        loss = criterion(out[train_msk], data.y[train_msk].float())\n",
    "        adv_loss = entropy_loss(turn_prob(out).log(), turn_prob(adv_out))\n",
    "        (loss + 0.5 * adv_loss).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += [loss.item()]\n",
    "        train_adv  += [adv_loss.item()]\n",
    "        \n",
    "        y_true += [data.y]\n",
    "        y_scores += [out]\n",
    "\n",
    "    input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "    train_metric = evaluator.eval(input_dict)[args.metric]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for num_iters, data in enumerate(tqdm(valid_loader)):\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr,  'rw': data.rw_edge_attr}\n",
    "            out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            loss = criterion(out, data.y.float())\n",
    "            valid_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        valid_metric = evaluator.eval(input_dict)[args.metric]\n",
    "        \n",
    "        test_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for data in test_loader:\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr,  'rw': data.rw_edge_attr}\n",
    "            out = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            loss = criterion(out, data.y.float())\n",
    "            test_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        test_metric = evaluator.eval(input_dict)[args.metric]\n",
    "\n",
    "    print('Epoch %d: LR: %.5f, Train loss: %.3f Train %s: %.3f Train Adv: %.3f Valid loss: %.3f  Valid %s: %.3f \\\n",
    "        Test loss: %.3f  Test %s: %.3f' \\\n",
    "          % (epoch + 1, optimizer.param_groups[0]['lr'], np.average(train_loss), args.metric, train_metric, \\\n",
    "             np.average(train_adv), np.average(valid_loss), args.metric, valid_metric, \\\n",
    "             np.average(test_loss), args.metric, test_metric))\n",
    "    stats += [[epoch, np.average(train_loss), train_metric, np.average(valid_loss), valid_metric, np.average(test_loss), test_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = ['epoch', 'train_loss', 'train_metric', 'valid_loss', 'valid_metric', 'test_loss', 'test_metric']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "stats_np = np.array(stats)\n",
    "best_valid = stats_np[stats_np[:50, 4].argmax()]\n",
    "print(best_valid)\n",
    "for i in range(1, stats_np.shape[-1]):\n",
    "    ax = fig.add_subplot(2, 3, i)\n",
    "    ax.plot(stats_np[:, i], label=labels[i])\n",
    "    ax.scatter(x=best_valid[0], y=best_valid[i], color='red')\n",
    "    ax.annotate(best_valid[i].round(3), xy=(best_valid[0]+5, best_valid[i]), color='red')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, to_networkx, dense_to_sparse, remove_self_loops, to_undirected\n",
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "\n",
    "model.eval()\n",
    "data = dataset[3]    \n",
    "data.to(args.device)\n",
    "strats = {'ea': data.edge_attr,  'hsd': data.hsd_edge_attr}\n",
    "out = model(data.x, 0, data.edge_index, strats)\n",
    "\n",
    "threshold = 0.95\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=len(model.gcs), ncols=2, figsize=(15, 10 * len(model.gcs)))\n",
    "for layer_idx, gc in enumerate(model.gcs):\n",
    "    imgs = []\n",
    "    \n",
    "    adj = to_dense_adj(edge_index=data.edge_index, edge_attr=gc.att)[0]\n",
    "    adj_mean = adj.mean(dim=-1).detach().cpu()\n",
    "    adj_max = adj.max(dim=-1)[0].detach().cpu()\n",
    "    \n",
    "    adj_mean_sorted = adj_mean.flatten().sort()[0]\n",
    "    adj_mean_threshold = adj_mean_sorted[int(threshold * len(adj_mean_sorted))]\n",
    "    adj_mean[adj_mean < adj_mean_threshold] = 0\n",
    "    adj_mean[adj_mean >= adj_mean_threshold] = 1\n",
    "    \n",
    "#     adj_max_sorted = adj_max.flatten().sort()[0]\n",
    "#     adj_max_threshold = adj_max_sorted[int(threshold * len(adj_max_sorted))]\n",
    "#     adj_max[adj_max < max_threshold] = 0\n",
    "#     adj_max[adj_max >= max_threshold] = 1\n",
    "    \n",
    "    mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "    mean_edge_index = remove_self_loops(mean_edge_index)[0]\n",
    "    mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "    \n",
    "#     max_edge_index = dense_to_sparse(adj_max.long())[0]\n",
    "#     max_edge_index = remove_self_loops(max_edge_index)[0]\n",
    "#     max_data = Data(x=data.x, edge_index=max_edge_index)\n",
    "    \n",
    "    # ax = axes[layer_idx][0]\n",
    "    # ax.set_title(f'Layer {layer_idx + 1}, Mean')\n",
    "    # im = ax.matshow(adj_mean)\n",
    "    # fig.colorbar(im, ax=ax)\n",
    "    # molecule_draw_with_color(to_networkx(mean_data, node_attrs=['x']), ax=ax, labels='node_id')\n",
    "    mol = pyg_to_mol(mean_data)  \n",
    "    # mc = mol_to_svg(mol, molSize=(150, 150))\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    \n",
    "    # ax = axes[layer_idx][1]\n",
    "    # ax.set_title(f'Layer {layer_idx + 1}, Max')\n",
    "    # im = ax.matshow(adj_max)\n",
    "    # fig.colorbar(im, ax=ax)\n",
    "    # molecule_draw_with_color(to_networkx(max_data, node_attrs=['x']), ax=ax, labels='node_id')\n",
    "    \n",
    "    # mol = pyg_to_mol(max_data)  \n",
    "    # mc = mol_to_svg(mol, molSize=(150, 150))\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    # imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
