{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from graph_transformer import GT\n",
    "from utils import pre_process, pre_process_with_summary, concat_pre_process_with_summary, inf_sum_pre_process_with_summary, fin_sum_pre_process_with_summary, get_n_params, get_optimizer\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import pytz\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, dense_to_sparse, remove_self_loops, to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.dataset = 'ogbg-molhiv'\n",
    "args.n_classes = 1\n",
    "args.lr = 3e-4\n",
    "args.n_hid = 512\n",
    "args.n_heads = 8\n",
    "args.n_layer = 4\n",
    "args.dropout = 0.3\n",
    "args.num_epochs = 50\n",
    "# args.num_epochs = 1\n",
    "args.k_hop_neighbors = 3\n",
    "args.k_hop = True\n",
    "args.weight_decay = 1e-2\n",
    "# args.bsz      = 512\n",
    "args.bsz      = 448\n",
    "args.strategies = ['ea', 'rw_concat']\n",
    "args.summary_node = True\n",
    "args.hier_levels = 3\n",
    "args.lap_k = None\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.temp = torch.nn.Linear(1, 1).to(args.device)\n",
    "args.metric = 'rocauc'\n",
    "print(\"device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-molhiv \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "tz = pytz.timezone('US/Pacific')\n",
    "time_now = datetime.datetime.now(tz).strftime('%m-%d_%H:%M:%S')\n",
    "\n",
    "if args.summary_node:\n",
    "    pre_transform = lambda d : concat_pre_process_with_summary(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/concat_with_summary_{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/concat_with_summary_{args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "\n",
    "else:\n",
    "    pre_transform = lambda d : pre_process(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "    \n",
    "    \n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=pre_transform, root = root_path)\n",
    "orig_dataset = PygGraphPropPredDataset(name=args.dataset)\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "split_idx = dataset.get_idx_split()\n",
    "edge_dim_dict = {'ea': None, \\\n",
    "                 'disc': {\n",
    "#                      'sd': (dataset.data.sd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'cn': (dataset.data.cn_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'hsd': (dataset.data.hsd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "                    },\n",
    "                 'cont': {\n",
    "                     **{('rw_' + str(k)): args.n_hid for k in range(1, args.k_hop_neighbors + 1)}\n",
    "#                      'rw': args.n_hid\n",
    "                 }\n",
    "                }\n",
    "model = GT(args.n_hid, args.n_classes, args.n_heads, args.n_layer, edge_dim_dict, args.dropout, args.summary_node, args.lap_k).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)\n",
    "\n",
    "orig_valid_loader = DataLoader(orig_dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "orig_test_loader  = DataLoader(orig_dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 10338817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Model #Params: %d' % get_n_params(model))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"sum\")\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay = args.weight_decay, learning_rate = args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "scheduler.step(-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "def mat_visualize(node_size, edge_index, edge_attr):\n",
    "    mat = np.zeros((node_size, node_size))\n",
    "    for e, v in zip(edge_index, edge_attr):\n",
    "        mat[e[0]][e[1]] = v\n",
    "    sb.heatmap(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_prob(inp):\n",
    "    prob = torch.sigmoid(inp)\n",
    "    prob = torch.cat([prob, 1-prob], dim=1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "valid_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "test_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "\n",
    "train_mask[split_idx[\"train\"]] = True\n",
    "valid_mask[split_idx[\"valid\"]] = True\n",
    "test_mask[split_idx[\"test\"]] = True\n",
    "def entropy_loss(pred, label):\n",
    "    return torch.mean(torch.sum(-label * pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5376, 0.4789, 0.4766, 0.4733, 0.4757, 0.4758, 0.4808, 0.4808, 0.4797,\n",
      "         0.4824, 0.4846, 0.4738, 0.4726, 0.4787, 0.4766, 0.4824, 0.4808, 0.4846,\n",
      "         0.4772],\n",
      "        [0.4789, 0.5376, 0.4904, 0.4914, 0.4952, 0.4951, 0.4971, 0.4949, 0.5017,\n",
      "         0.4968, 0.4964, 0.5003, 0.4901, 0.4967, 0.4927, 0.4944, 0.4974, 0.4958,\n",
      "         0.5028],\n",
      "        [0.4766, 0.4904, 0.5376, 0.4989, 0.4999, 0.4990, 0.4969, 0.5046, 0.4967,\n",
      "         0.4991, 0.4948, 0.4955, 0.5020, 0.4973, 0.4950, 0.5010, 0.5001, 0.5037,\n",
      "         0.4931],\n",
      "        [0.4733, 0.4914, 0.4989, 0.5376, 0.5056, 0.5004, 0.5013, 0.5015, 0.4973,\n",
      "         0.5030, 0.4950, 0.4894, 0.4991, 0.5020, 0.5051, 0.5033, 0.5008, 0.5021,\n",
      "         0.4930],\n",
      "        [0.4757, 0.4952, 0.4999, 0.5056, 0.5376, 0.5078, 0.5052, 0.5060, 0.4979,\n",
      "         0.5030, 0.5000, 0.4950, 0.4966, 0.5034, 0.5042, 0.5054, 0.5041, 0.5011,\n",
      "         0.4962],\n",
      "        [0.4758, 0.4951, 0.4990, 0.5004, 0.5078, 0.5376, 0.5055, 0.4988, 0.4961,\n",
      "         0.5022, 0.4995, 0.4907, 0.5008, 0.5061, 0.5020, 0.5048, 0.5041, 0.4988,\n",
      "         0.4996],\n",
      "        [0.4808, 0.4971, 0.4969, 0.5013, 0.5052, 0.5055, 0.5376, 0.4979, 0.4950,\n",
      "         0.5047, 0.4975, 0.4891, 0.4998, 0.5070, 0.5035, 0.5050, 0.5040, 0.4999,\n",
      "         0.4988],\n",
      "        [0.4808, 0.4949, 0.5046, 0.5015, 0.5060, 0.4988, 0.4979, 0.5376, 0.4958,\n",
      "         0.5018, 0.5009, 0.4942, 0.5026, 0.4990, 0.4983, 0.5043, 0.5040, 0.5071,\n",
      "         0.4978],\n",
      "        [0.4797, 0.5017, 0.4967, 0.4973, 0.4979, 0.4961, 0.4950, 0.4958, 0.5376,\n",
      "         0.5010, 0.4973, 0.5052, 0.4985, 0.4951, 0.4922, 0.4975, 0.4960, 0.5001,\n",
      "         0.5033],\n",
      "        [0.4824, 0.4968, 0.4991, 0.5030, 0.5030, 0.5022, 0.5047, 0.5018, 0.5010,\n",
      "         0.5376, 0.5036, 0.4963, 0.5004, 0.5070, 0.5034, 0.5066, 0.5070, 0.5045,\n",
      "         0.5023],\n",
      "        [0.4846, 0.4964, 0.4948, 0.4950, 0.5000, 0.4995, 0.4975, 0.5009, 0.4973,\n",
      "         0.5036, 0.5376, 0.4983, 0.4940, 0.4998, 0.5002, 0.5003, 0.4984, 0.4994,\n",
      "         0.5015],\n",
      "        [0.4738, 0.5003, 0.4955, 0.4894, 0.4950, 0.4907, 0.4891, 0.4942, 0.5052,\n",
      "         0.4963, 0.4983, 0.5376, 0.4930, 0.4915, 0.4890, 0.4919, 0.4948, 0.4946,\n",
      "         0.5008],\n",
      "        [0.4726, 0.4901, 0.5020, 0.4991, 0.4966, 0.5008, 0.4998, 0.5026, 0.4985,\n",
      "         0.5004, 0.4940, 0.4930, 0.5376, 0.4991, 0.5008, 0.5049, 0.5024, 0.5027,\n",
      "         0.4948],\n",
      "        [0.4787, 0.4967, 0.4973, 0.5020, 0.5034, 0.5061, 0.5070, 0.4990, 0.4951,\n",
      "         0.5070, 0.4998, 0.4915, 0.4991, 0.5376, 0.5072, 0.5082, 0.5092, 0.5013,\n",
      "         0.5011],\n",
      "        [0.4766, 0.4927, 0.4950, 0.5051, 0.5042, 0.5020, 0.5035, 0.4983, 0.4922,\n",
      "         0.5034, 0.5002, 0.4890, 0.5008, 0.5072, 0.5376, 0.5058, 0.5081, 0.5010,\n",
      "         0.4952],\n",
      "        [0.4824, 0.4944, 0.5010, 0.5033, 0.5054, 0.5048, 0.5050, 0.5043, 0.4975,\n",
      "         0.5066, 0.5003, 0.4919, 0.5049, 0.5082, 0.5058, 0.5376, 0.5069, 0.5061,\n",
      "         0.4953],\n",
      "        [0.4808, 0.4974, 0.5001, 0.5008, 0.5041, 0.5041, 0.5040, 0.5040, 0.4960,\n",
      "         0.5070, 0.4984, 0.4948, 0.5024, 0.5092, 0.5081, 0.5069, 0.5376, 0.5042,\n",
      "         0.5001],\n",
      "        [0.4846, 0.4958, 0.5037, 0.5021, 0.5011, 0.4988, 0.4999, 0.5071, 0.5001,\n",
      "         0.5045, 0.4994, 0.4946, 0.5027, 0.5013, 0.5010, 0.5061, 0.5042, 0.5376,\n",
      "         0.4987],\n",
      "        [0.4772, 0.5028, 0.4931, 0.4930, 0.4962, 0.4996, 0.4988, 0.4978, 0.5033,\n",
      "         0.5023, 0.5015, 0.5008, 0.4948, 0.5011, 0.4952, 0.4953, 0.5001, 0.4987,\n",
      "         0.5376]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/91 [00:02<03:11,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5376, 0.4968, 0.4975, 0.5005, 0.5021, 0.5020, 0.4930, 0.4897, 0.4863,\n",
      "         0.4917, 0.4986, 0.4855, 0.4887, 0.4874, 0.4888, 0.5049, 0.4893, 0.4992,\n",
      "         0.4840, 0.4934, 0.4913, 0.4890, 0.5030, 0.4878, 0.5013, 0.4879],\n",
      "        [0.4968, 0.5376, 0.5035, 0.5076, 0.5019, 0.5048, 0.4925, 0.4858, 0.4832,\n",
      "         0.4937, 0.5025, 0.4847, 0.4935, 0.4877, 0.4894, 0.4988, 0.4858, 0.4991,\n",
      "         0.4876, 0.4963, 0.4898, 0.4872, 0.4971, 0.4877, 0.4975, 0.4830],\n",
      "        [0.4975, 0.5035, 0.5376, 0.5055, 0.5051, 0.5011, 0.4961, 0.4913, 0.4859,\n",
      "         0.4915, 0.5001, 0.4913, 0.4944, 0.4887, 0.4916, 0.4945, 0.4875, 0.5025,\n",
      "         0.4894, 0.4940, 0.4878, 0.4886, 0.4960, 0.4884, 0.4940, 0.4821],\n",
      "        [0.5005, 0.5076, 0.5055, 0.5376, 0.5056, 0.5028, 0.4954, 0.4890, 0.4859,\n",
      "         0.4958, 0.5029, 0.4920, 0.4920, 0.4876, 0.4926, 0.4960, 0.4866, 0.5018,\n",
      "         0.4890, 0.4958, 0.4915, 0.4899, 0.5022, 0.4876, 0.4954, 0.4839],\n",
      "        [0.5021, 0.5019, 0.5051, 0.5056, 0.5376, 0.5048, 0.4928, 0.4955, 0.4863,\n",
      "         0.4950, 0.5050, 0.4888, 0.4970, 0.4892, 0.4937, 0.4963, 0.4889, 0.5026,\n",
      "         0.4909, 0.4925, 0.4913, 0.4920, 0.5000, 0.4919, 0.4989, 0.4844],\n",
      "        [0.5020, 0.5048, 0.5011, 0.5028, 0.5048, 0.5376, 0.5013, 0.4917, 0.4880,\n",
      "         0.4907, 0.5034, 0.4914, 0.4955, 0.4864, 0.4911, 0.4997, 0.4882, 0.5033,\n",
      "         0.4923, 0.4961, 0.4914, 0.4863, 0.5006, 0.4893, 0.5014, 0.4855],\n",
      "        [0.4930, 0.4925, 0.4961, 0.4954, 0.4928, 0.5013, 0.5376, 0.4892, 0.4911,\n",
      "         0.4882, 0.4931, 0.4987, 0.4977, 0.4981, 0.4913, 0.4935, 0.4886, 0.4968,\n",
      "         0.5006, 0.4957, 0.4968, 0.4896, 0.4973, 0.4886, 0.4948, 0.4819],\n",
      "        [0.4897, 0.4858, 0.4913, 0.4890, 0.4955, 0.4917, 0.4892, 0.5376, 0.5006,\n",
      "         0.5080, 0.4865, 0.4952, 0.5025, 0.4930, 0.4910, 0.4861, 0.5032, 0.4870,\n",
      "         0.4919, 0.4955, 0.4941, 0.4881, 0.4863, 0.5020, 0.4918, 0.4755],\n",
      "        [0.4863, 0.4832, 0.4859, 0.4859, 0.4863, 0.4880, 0.4911, 0.5006, 0.5376,\n",
      "         0.4968, 0.4833, 0.4951, 0.4931, 0.4951, 0.4979, 0.4868, 0.5015, 0.4875,\n",
      "         0.4973, 0.4887, 0.4969, 0.4951, 0.4854, 0.5007, 0.4849, 0.4793],\n",
      "        [0.4917, 0.4937, 0.4915, 0.4958, 0.4950, 0.4907, 0.4882, 0.5080, 0.4968,\n",
      "         0.5376, 0.4953, 0.4982, 0.4985, 0.4929, 0.4945, 0.4919, 0.5057, 0.4933,\n",
      "         0.4893, 0.4964, 0.4940, 0.4908, 0.4915, 0.5015, 0.4945, 0.4753],\n",
      "        [0.4986, 0.5025, 0.5001, 0.5029, 0.5050, 0.5034, 0.4931, 0.4865, 0.4833,\n",
      "         0.4953, 0.5376, 0.4899, 0.4960, 0.4856, 0.4893, 0.5000, 0.4906, 0.5059,\n",
      "         0.4852, 0.4902, 0.4859, 0.4909, 0.4993, 0.4897, 0.4965, 0.4816],\n",
      "        [0.4855, 0.4847, 0.4913, 0.4920, 0.4888, 0.4914, 0.4987, 0.4952, 0.4951,\n",
      "         0.4982, 0.4899, 0.5376, 0.4964, 0.4975, 0.4934, 0.4887, 0.4913, 0.4892,\n",
      "         0.5021, 0.4932, 0.4979, 0.4913, 0.4945, 0.4922, 0.4917, 0.4795],\n",
      "        [0.4887, 0.4935, 0.4944, 0.4920, 0.4970, 0.4955, 0.4977, 0.5025, 0.4931,\n",
      "         0.4985, 0.4960, 0.4964, 0.5376, 0.4992, 0.4971, 0.4927, 0.4944, 0.4935,\n",
      "         0.4975, 0.4981, 0.4957, 0.4918, 0.4919, 0.4934, 0.4973, 0.4800],\n",
      "        [0.4874, 0.4877, 0.4887, 0.4876, 0.4892, 0.4864, 0.4981, 0.4930, 0.4951,\n",
      "         0.4929, 0.4856, 0.4975, 0.4992, 0.5376, 0.4963, 0.4918, 0.4918, 0.4879,\n",
      "         0.4984, 0.4958, 0.5053, 0.4908, 0.4930, 0.4882, 0.4911, 0.4813],\n",
      "        [0.4888, 0.4894, 0.4916, 0.4926, 0.4937, 0.4911, 0.4913, 0.4910, 0.4979,\n",
      "         0.4945, 0.4893, 0.4934, 0.4971, 0.4963, 0.5376, 0.4904, 0.4952, 0.4909,\n",
      "         0.4956, 0.4980, 0.4999, 0.5029, 0.4946, 0.4964, 0.4938, 0.4842],\n",
      "        [0.5049, 0.4988, 0.4945, 0.4960, 0.4963, 0.4997, 0.4935, 0.4861, 0.4868,\n",
      "         0.4919, 0.5000, 0.4887, 0.4927, 0.4918, 0.4904, 0.5376, 0.4898, 0.4989,\n",
      "         0.4907, 0.4952, 0.4923, 0.4895, 0.5031, 0.4879, 0.4985, 0.4868],\n",
      "        [0.4893, 0.4858, 0.4875, 0.4866, 0.4889, 0.4882, 0.4886, 0.5032, 0.5015,\n",
      "         0.5057, 0.4906, 0.4913, 0.4944, 0.4918, 0.4952, 0.4898, 0.5376, 0.4878,\n",
      "         0.4922, 0.4938, 0.4959, 0.4962, 0.4903, 0.5034, 0.4907, 0.4760],\n",
      "        [0.4992, 0.4991, 0.5025, 0.5018, 0.5026, 0.5033, 0.4968, 0.4870, 0.4875,\n",
      "         0.4933, 0.5059, 0.4892, 0.4935, 0.4879, 0.4909, 0.4989, 0.4878, 0.5376,\n",
      "         0.4900, 0.4938, 0.4932, 0.4878, 0.5012, 0.4867, 0.4999, 0.4863],\n",
      "        [0.4840, 0.4876, 0.4894, 0.4890, 0.4909, 0.4923, 0.5006, 0.4919, 0.4973,\n",
      "         0.4893, 0.4852, 0.5021, 0.4975, 0.4984, 0.4956, 0.4907, 0.4922, 0.4900,\n",
      "         0.5376, 0.4988, 0.4976, 0.4897, 0.4932, 0.4923, 0.4928, 0.4822],\n",
      "        [0.4934, 0.4963, 0.4940, 0.4958, 0.4925, 0.4961, 0.4957, 0.4955, 0.4887,\n",
      "         0.4964, 0.4902, 0.4932, 0.4981, 0.4958, 0.4980, 0.4952, 0.4938, 0.4938,\n",
      "         0.4988, 0.5376, 0.4985, 0.4940, 0.4970, 0.4908, 0.4940, 0.4768],\n",
      "        [0.4913, 0.4898, 0.4878, 0.4915, 0.4913, 0.4914, 0.4968, 0.4941, 0.4969,\n",
      "         0.4940, 0.4859, 0.4979, 0.4957, 0.5053, 0.4999, 0.4923, 0.4959, 0.4932,\n",
      "         0.4976, 0.4985, 0.5376, 0.4934, 0.4986, 0.4932, 0.4965, 0.4832],\n",
      "        [0.4890, 0.4872, 0.4886, 0.4899, 0.4920, 0.4863, 0.4896, 0.4881, 0.4951,\n",
      "         0.4908, 0.4909, 0.4913, 0.4918, 0.4908, 0.5029, 0.4895, 0.4962, 0.4878,\n",
      "         0.4897, 0.4940, 0.4934, 0.5376, 0.4922, 0.4951, 0.4857, 0.4780],\n",
      "        [0.5030, 0.4971, 0.4960, 0.5022, 0.5000, 0.5006, 0.4973, 0.4863, 0.4854,\n",
      "         0.4915, 0.4993, 0.4945, 0.4919, 0.4930, 0.4946, 0.5031, 0.4903, 0.5012,\n",
      "         0.4932, 0.4970, 0.4986, 0.4922, 0.5376, 0.4867, 0.5019, 0.4884],\n",
      "        [0.4878, 0.4877, 0.4884, 0.4876, 0.4919, 0.4893, 0.4886, 0.5020, 0.5007,\n",
      "         0.5015, 0.4897, 0.4922, 0.4934, 0.4882, 0.4964, 0.4879, 0.5034, 0.4867,\n",
      "         0.4923, 0.4908, 0.4932, 0.4951, 0.4867, 0.5376, 0.4862, 0.4747],\n",
      "        [0.5013, 0.4975, 0.4940, 0.4954, 0.4989, 0.5014, 0.4948, 0.4918, 0.4849,\n",
      "         0.4945, 0.4965, 0.4917, 0.4973, 0.4911, 0.4938, 0.4985, 0.4907, 0.4999,\n",
      "         0.4928, 0.4940, 0.4965, 0.4857, 0.5019, 0.4862, 0.5376, 0.4834],\n",
      "        [0.4879, 0.4830, 0.4821, 0.4839, 0.4844, 0.4855, 0.4819, 0.4755, 0.4793,\n",
      "         0.4753, 0.4816, 0.4795, 0.4800, 0.4813, 0.4842, 0.4868, 0.4760, 0.4863,\n",
      "         0.4822, 0.4768, 0.4832, 0.4780, 0.4884, 0.4747, 0.4834, 0.5376]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/91 [00:04<03:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5376, 0.4920, 0.4927, 0.4947, 0.4972, 0.5006, 0.5005, 0.4918, 0.4946,\n",
      "         0.4943, 0.4952, 0.4939, 0.4947, 0.4980, 0.4980],\n",
      "        [0.4920, 0.5376, 0.4941, 0.4939, 0.4956, 0.4924, 0.4942, 0.4898, 0.4915,\n",
      "         0.4885, 0.4874, 0.4924, 0.4888, 0.4895, 0.4908],\n",
      "        [0.4927, 0.4941, 0.5376, 0.4955, 0.4996, 0.4959, 0.4887, 0.4853, 0.4889,\n",
      "         0.4852, 0.4871, 0.4847, 0.4827, 0.4897, 0.4876],\n",
      "        [0.4947, 0.4939, 0.4955, 0.5376, 0.5038, 0.4967, 0.4959, 0.4915, 0.4925,\n",
      "         0.4909, 0.4921, 0.4944, 0.4914, 0.4915, 0.4942],\n",
      "        [0.4972, 0.4956, 0.4996, 0.5038, 0.5376, 0.4960, 0.4938, 0.4915, 0.4939,\n",
      "         0.4964, 0.4922, 0.4928, 0.4910, 0.4928, 0.4925],\n",
      "        [0.5006, 0.4924, 0.4959, 0.4967, 0.4960, 0.5376, 0.5030, 0.5007, 0.5032,\n",
      "         0.5034, 0.5002, 0.5038, 0.5013, 0.5037, 0.5003],\n",
      "        [0.5005, 0.4942, 0.4887, 0.4959, 0.4938, 0.5030, 0.5376, 0.4992, 0.5027,\n",
      "         0.4998, 0.5039, 0.5018, 0.5046, 0.5027, 0.5033],\n",
      "        [0.4918, 0.4898, 0.4853, 0.4915, 0.4915, 0.5007, 0.4992, 0.5376, 0.5034,\n",
      "         0.5020, 0.5016, 0.5014, 0.4997, 0.4961, 0.5029],\n",
      "        [0.4946, 0.4915, 0.4889, 0.4925, 0.4939, 0.5032, 0.5027, 0.5034, 0.5376,\n",
      "         0.5033, 0.5063, 0.5043, 0.5060, 0.5037, 0.5063],\n",
      "        [0.4943, 0.4885, 0.4852, 0.4909, 0.4964, 0.5034, 0.4998, 0.5020, 0.5033,\n",
      "         0.5376, 0.5033, 0.5028, 0.5028, 0.5000, 0.5020],\n",
      "        [0.4952, 0.4874, 0.4871, 0.4921, 0.4922, 0.5002, 0.5039, 0.5016, 0.5063,\n",
      "         0.5033, 0.5376, 0.5009, 0.5061, 0.5041, 0.5047],\n",
      "        [0.4939, 0.4924, 0.4847, 0.4944, 0.4928, 0.5038, 0.5018, 0.5014, 0.5043,\n",
      "         0.5028, 0.5009, 0.5376, 0.5064, 0.5029, 0.5032],\n",
      "        [0.4947, 0.4888, 0.4827, 0.4914, 0.4910, 0.5013, 0.5046, 0.4997, 0.5060,\n",
      "         0.5028, 0.5061, 0.5064, 0.5376, 0.5042, 0.5057],\n",
      "        [0.4980, 0.4895, 0.4897, 0.4915, 0.4928, 0.5037, 0.5027, 0.4961, 0.5037,\n",
      "         0.5000, 0.5041, 0.5029, 0.5042, 0.5376, 0.5029],\n",
      "        [0.4980, 0.4908, 0.4876, 0.4942, 0.4925, 0.5003, 0.5033, 0.5029, 0.5063,\n",
      "         0.5020, 0.5047, 0.5032, 0.5057, 0.5029, 0.5376]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/91 [00:05<02:50,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5376, 0.4996, 0.4868,  ..., 0.4844, 0.5001, 0.4955],\n",
      "        [0.4996, 0.5376, 0.4927,  ..., 0.4889, 0.5044, 0.4964],\n",
      "        [0.4868, 0.4927, 0.5376,  ..., 0.4975, 0.4872, 0.4848],\n",
      "        ...,\n",
      "        [0.4844, 0.4889, 0.4975,  ..., 0.5376, 0.4859, 0.4885],\n",
      "        [0.5001, 0.5044, 0.4872,  ..., 0.4859, 0.5376, 0.4963],\n",
      "        [0.4955, 0.4964, 0.4848,  ..., 0.4885, 0.4963, 0.5376]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/91 [00:07<02:42,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5376, 0.4910, 0.4885,  ..., 0.4916, 0.5038, 0.4893],\n",
      "        [0.4910, 0.5376, 0.4969,  ..., 0.4981, 0.4894, 0.4959],\n",
      "        [0.4885, 0.4969, 0.5376,  ..., 0.4979, 0.4937, 0.5022],\n",
      "        ...,\n",
      "        [0.4916, 0.4981, 0.4979,  ..., 0.5376, 0.4938, 0.4929],\n",
      "        [0.5038, 0.4894, 0.4937,  ..., 0.4938, 0.5376, 0.4908],\n",
      "        [0.4893, 0.4959, 0.5022,  ..., 0.4929, 0.4908, 0.5376]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/91 [00:10<02:52,  2.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d304bd43f89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnormalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mpred_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_adj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mnumel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_adv  = []\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    all_idx = torch.randperm(len(dataset))\n",
    "    for batch_idx in tqdm(range(len(all_idx) // args.bsz)):\n",
    "        batch = all_idx[batch_idx * args.bsz : (batch_idx + 1) * args.bsz]\n",
    "        train_msk = train_mask[batch]    \n",
    "        data = Batch.from_data_list(dataset[batch])\n",
    "        data.to(args.device)\n",
    "        \n",
    "        strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "#         strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "        out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        with torch.no_grad():\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            adv_out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "        # loss = criterion(out[train_msk], data.y[train_msk].float())\n",
    "        loss = 0.0\n",
    "        numel = 0\n",
    "        for idx, orig_data in enumerate(orig_dataset[batch]):\n",
    "            if not train_msk[idx]:\n",
    "                continue\n",
    "            orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "            normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "            pred_adj = args.temp(torch.mm(normalized, normalized.t()).unsqueeze(-1)).squeeze(-1)\n",
    "            loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "            numel += pred_adj.numel()\n",
    "        print(pred_adj.sigmoid())\n",
    "        loss = loss / numel\n",
    "        \n",
    "        adv_loss = entropy_loss(turn_prob(out[train_msk]).log(), turn_prob(adv_out[train_msk]))\n",
    "        adv_loss = adv_loss * 0\n",
    "        (loss + 0.5 * adv_loss).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += [loss.item()]\n",
    "        train_adv  += [adv_loss.item()]\n",
    "        \n",
    "        y_true += [data.y]\n",
    "        y_scores += [out]\n",
    "\n",
    "    input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "    train_metric = evaluator.eval(input_dict)[args.metric]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for num_iters, (data, orig_data_batch) in enumerate(tqdm(zip(valid_loader, orig_valid_loader))):\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            # loss = criterion(out, data.y.float())\n",
    "            loss = 0.0\n",
    "            numel = 0\n",
    "            for idx, orig_data in enumerate(orig_data_batch.to_data_list()):\n",
    "                orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "                normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "                pred_adj = args.temp(torch.mm(normalized, normalized.t()).unsqueeze(-1)).squeeze(-1)\n",
    "                loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "                numel += pred_adj.numel()\n",
    "            loss = loss / numel\n",
    "            \n",
    "            valid_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        valid_metric = evaluator.eval(input_dict)[args.metric]\n",
    "        \n",
    "        test_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for (data, orig_data_batch) in zip(test_loader, orig_test_loader):\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            # loss = criterion(out, data.y.float())\n",
    "            loss = 0.0\n",
    "            numel = 0\n",
    "            for idx, orig_data in enumerate(orig_data_batch.to_data_list()):\n",
    "                orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "                normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "                pred_adj = args.temp(torch.mm(normalized, normalized.t()).unsqueeze(-1)).squeeze(-1)\n",
    "                loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "                numel += pred_adj.numel()\n",
    "            loss = loss / numel\n",
    "        \n",
    "            test_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        test_metric = evaluator.eval(input_dict)[args.metric]\n",
    "\n",
    "    print('Epoch %d: LR: %.5f, Train loss: %.3f Train %s: %.3f Train Adv: %.3f Valid loss: %.3f  Valid %s: %.3f \\\n",
    "        Test loss: %.3f  Test %s: %.3f' \\\n",
    "          % (epoch + 1, optimizer.param_groups[0]['lr'], np.average(train_loss), args.metric, train_metric, \\\n",
    "             np.average(train_adv), np.average(valid_loss), args.metric, valid_metric, \\\n",
    "             np.average(test_loss), args.metric, test_metric))\n",
    "    stats += [[epoch, np.average(train_loss), train_metric, np.average(valid_loss), valid_metric, np.average(test_loss), test_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.temp.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = ['epoch', 'train_loss', 'train_metric', 'valid_loss', 'valid_metric', 'test_loss', 'test_metric']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "stats_np = np.array(stats)\n",
    "best_valid = stats_np[stats_np[:50, 4].argmax()]\n",
    "print(best_valid)\n",
    "for i in range(1, stats_np.shape[-1]):\n",
    "    ax = fig.add_subplot(2, 3, i)\n",
    "    ax.plot(stats_np[:, i], label=labels[i])\n",
    "    ax.scatter(x=best_valid[0], y=best_valid[i], color='red')\n",
    "    ax.annotate(best_valid[i].round(3), xy=(best_valid[0]+5, best_valid[i]), color='red')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.utils import degree\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "\n",
    "def generateSequence(startIndex, transitionMatrix, path_length, alpha):\n",
    "    result = [startIndex]\n",
    "    current = startIndex\n",
    "\n",
    "    for i in range(0, path_length):\n",
    "        if random.random() < alpha:\n",
    "            nextIndex = startIndex\n",
    "        else:\n",
    "            probs = transitionMatrix[current]\n",
    "            assert np.sum(probs) != 0, print(probs)\n",
    "            nextIndex = np.random.choice(len(probs), 1, p=probs)[0]\n",
    "\n",
    "        result.append(nextIndex)\n",
    "        current = nextIndex\n",
    "\n",
    "    return result\n",
    "\n",
    "def weighted_random_walk(data, transitionMatrix, path_length, alpha, degree_weighted_start=True, num_samples=3):\n",
    "    if degree_weighted_start:\n",
    "        # Exclude degree 1 nodes, soft max over remaining degrees\n",
    "        p = degree(data.edge_index[0])\n",
    "        p[p == 1] = 0\n",
    "    else:\n",
    "        p = torch.ones(data.num_nodes)    \n",
    "    m = Multinomial(num_samples, probs=p.exp()-1)\n",
    "    start_node = m.sample().long().tolist()\n",
    "    start = torch.Tensor(sum([[i] * start_node[i] for i in range(len(start_node))], [])).long()\n",
    "    \n",
    "    sentenceList = []\n",
    "    nodes = list(range(data.num_nodes))\n",
    "    \n",
    "    for j in range(0, num_samples):\n",
    "        indexList = generateSequence(start[j].item(), transitionMatrix, path_length, alpha)\n",
    "        sentence = [nodes[tmp] for tmp in indexList]\n",
    "        sentence = torch.LongTensor(sentence).unique()\n",
    "        sentenceList.append(sentence)\n",
    "\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "\n",
    "model.eval()\n",
    "# threshold = 0.1\n",
    "\n",
    "for idx in range(20, 41):\n",
    "    orig_data = orig_dataset[idx]\n",
    "\n",
    "    data = dataset[idx]\n",
    "    data.to(args.device)\n",
    "    strats = {'ea': data.edge_attr,  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "    out, _ = model(data.x, 0, data.edge_index, strats)\n",
    "    \n",
    "    imgs = []\n",
    "    for layer_idx, gc in enumerate(model.gcs):\n",
    "\n",
    "        adj = to_dense_adj(edge_index=data.edge_index, edge_attr=gc.att)[0]\n",
    "        adj_mean = adj.mean(dim=-1).detach().cpu()\n",
    "        adj_mean = adj_mean[:-1, :-1] # remove virtual node\n",
    "        \n",
    "        # only include edges that were in the original graph\n",
    "#         orig_adj = to_dense_adj(edge_index=orig_data.edge_index)[0].bool()\n",
    "#         adj_mean[~orig_adj] = 0\n",
    "        adj_mean /= adj_mean.sum(dim=1, keepdim=True)\n",
    "        \n",
    "#         adj_mean_sorted = adj_mean.flatten().sort()[0]\n",
    "#         adj_mean_sorted = adj_mean_sorted[adj_mean_sorted != 0]\n",
    "#         adj_mean_threshold = adj_mean_sorted[int(threshold * len(adj_mean_sorted))]\n",
    "#         adj_mean[adj_mean < adj_mean_threshold] = 0\n",
    "#         adj_mean[adj_mean >= adj_mean_threshold] = 1\n",
    "        subsetList = weighted_random_walk(orig_data, adj_mean.cpu().numpy(), 10, 0)\n",
    "        edgeIndexList = []\n",
    "        for s in subsetList:\n",
    "            edgeIndexList.append(torch.LongTensor(s).repeat_interleave(2)[1:-1].reshape(-1, 2))\n",
    "        mean_edge_index = sort_edge_index(torch.cat(edgeIndexList).t().contiguous())[0]\n",
    "    \n",
    "        # mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "        mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "\n",
    "        mol = pyg_to_mol(mean_data)  \n",
    "        svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "        imgs += [svg]\n",
    "\n",
    "    mol = pyg_to_mol(orig_data)\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "\n",
    "model.eval()\n",
    "# threshold = 0.0\n",
    "orig_dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
    "\n",
    "for idx in range(20, 41):\n",
    "    orig_data = orig_dataset[idx]\n",
    "\n",
    "    data = dataset[idx]\n",
    "    data.to(args.device)\n",
    "    strats = {'ea': data.edge_attr,  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "    out, reps = model(data.x, 0, data.edge_index, strats)\n",
    "    \n",
    "    imgs = []\n",
    "    normalized = F.normalize(reps[:-1])\n",
    "    adj_mean = args.temp(torch.mm(normalized, normalized.t()))\n",
    "    adj_mean = adj_mean.sigmoid()\n",
    "    adj_mean[adj_mean >= 0.5] = 1\n",
    "    adj_mean[adj_mean < 0.5] = 0\n",
    "    adj_mean -= torch.eye(adj_mean.size(0), device=adj_mean.device)\n",
    "\n",
    "    mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "    mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "\n",
    "    mol = pyg_to_mol(mean_data)  \n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "\n",
    "    mol = pyg_to_mol(orig_data)\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
