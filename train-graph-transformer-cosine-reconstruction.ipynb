{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import ogb\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from graph_transformer import GT\n",
    "from utils import pre_process, pre_process_with_summary, concat_pre_process_with_summary, inf_sum_pre_process_with_summary, fin_sum_pre_process_with_summary, get_n_params, get_optimizer\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import pytz\n",
    "from torch_geometric.utils import to_dense_adj, to_networkx, dense_to_sparse, remove_self_loops, to_undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch implementation of relative positional encodings and relation-aware self-attention for graph Transformers')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.dataset = 'ogbg-molhiv'\n",
    "args.n_classes = 1\n",
    "args.lr = 3e-4\n",
    "args.n_hid = 512\n",
    "args.n_heads = 8\n",
    "args.n_layer = 4\n",
    "args.dropout = 0.3\n",
    "args.num_epochs = 50\n",
    "# args.num_epochs = 1\n",
    "args.k_hop_neighbors = 3\n",
    "args.k_hop = True\n",
    "args.weight_decay = 1e-2\n",
    "# args.bsz      = 512\n",
    "args.bsz      = 448\n",
    "args.strategies = ['ea', 'rw_concat']\n",
    "args.summary_node = True\n",
    "args.hier_levels = 3\n",
    "args.lap_k = None\n",
    "args.temp = 0.1\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.metric = 'rocauc'\n",
    "print(\"device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset: ogbg-molhiv \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "print(\"dataset: {} \".format(args.dataset))\n",
    "tz = pytz.timezone('US/Pacific')\n",
    "time_now = datetime.datetime.now(tz).strftime('%m-%d_%H:%M:%S')\n",
    "\n",
    "if args.summary_node:\n",
    "    pre_transform = lambda d : concat_pre_process_with_summary(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/concat_with_summary_{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/concat_with_summary_{args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "\n",
    "else:\n",
    "    pre_transform = lambda d : pre_process(d, args)\n",
    "    root_path= f'dataset/{args.dataset}/{args.k_hop_neighbors}'\n",
    "    # args.writer = SummaryWriter(log_dir=f'runs_new/{args.dataset}/k={args.k_hop_neighbors}/strats={\"-\".join(args.strategies)}/{time_now}')\n",
    "    \n",
    "    \n",
    "dataset = PygGraphPropPredDataset(name=args.dataset, pre_transform=pre_transform, root = root_path)\n",
    "orig_dataset = PygGraphPropPredDataset(name=args.dataset)\n",
    "evaluator = Evaluator(name=args.dataset)\n",
    "split_idx = dataset.get_idx_split()\n",
    "edge_dim_dict = {'ea': None, \\\n",
    "                 'disc': {\n",
    "#                      'sd': (dataset.data.sd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'cn': (dataset.data.cn_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "#                      'hsd': (dataset.data.hsd_edge_attr.max(dim=0)[0].int().view(-1) + 1).tolist(), \\\n",
    "                    },\n",
    "                 'cont': {\n",
    "                     **{('rw_' + str(k)): args.n_hid for k in range(1, args.k_hop_neighbors + 1)}\n",
    "#                      'rw': args.n_hid\n",
    "                 }\n",
    "                }\n",
    "model = GT(args.n_hid, args.n_classes, args.n_heads, args.n_layer, edge_dim_dict, args.dropout, args.summary_node, args.lap_k).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)\n",
    "\n",
    "orig_valid_loader = DataLoader(orig_dataset[split_idx[\"valid\"]], batch_size=args.bsz, shuffle = False)\n",
    "orig_test_loader  = DataLoader(orig_dataset[split_idx[\"test\"]],  batch_size=args.bsz, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #Params: 8241665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Model #Params: %d' % get_n_params(model))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"sum\")\n",
    "\n",
    "optimizer = get_optimizer(model, weight_decay = args.weight_decay, learning_rate = args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "scheduler.step(-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "def mat_visualize(node_size, edge_index, edge_attr):\n",
    "    mat = np.zeros((node_size, node_size))\n",
    "    for e, v in zip(edge_index, edge_attr):\n",
    "        mat[e[0]][e[1]] = v\n",
    "    sb.heatmap(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_prob(inp):\n",
    "    prob = torch.sigmoid(inp)\n",
    "    prob = torch.cat([prob, 1-prob], dim=1)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "valid_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "test_mask = torch.zeros(len(dataset), dtype=bool)\n",
    "\n",
    "train_mask[split_idx[\"train\"]] = True\n",
    "valid_mask[split_idx[\"valid\"]] = True\n",
    "test_mask[split_idx[\"test\"]] = True\n",
    "def entropy_loss(pred, label):\n",
    "    return torch.mean(torch.sum(-label * pred, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:26<00:00,  1.61s/it]\n",
      "10it [00:10,  1.09s/it]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: LR: 0.00002, Train loss: 4.016 Train rocauc: 0.496 Train Adv: 0.000 Valid loss: 7.066  Valid rocauc: 0.489         Test loss: 7.015  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:22<00:00,  1.56s/it]\n",
      "10it [00:09,  1.09it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: LR: 0.00009, Train loss: 4.015 Train rocauc: 0.494 Train Adv: 0.000 Valid loss: 7.066  Valid rocauc: 0.489         Test loss: 7.015  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:19<00:00,  1.53s/it]\n",
      "10it [00:09,  1.08it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: LR: 0.00017, Train loss: 4.013 Train rocauc: 0.484 Train Adv: 0.000 Valid loss: 7.066  Valid rocauc: 0.489         Test loss: 7.015  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:22<00:00,  1.57s/it]\n",
      "10it [00:09,  1.03it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: LR: 0.00025, Train loss: 4.014 Train rocauc: 0.489 Train Adv: 0.000 Valid loss: 7.065  Valid rocauc: 0.489         Test loss: 7.014  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:22<00:00,  1.57s/it]\n",
      "10it [00:11,  1.10s/it]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: LR: 0.00029, Train loss: 4.013 Train rocauc: 0.496 Train Adv: 0.000 Valid loss: 7.065  Valid rocauc: 0.489         Test loss: 7.014  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:29<00:00,  1.64s/it]\n",
      "10it [00:09,  1.03it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: LR: 0.00029, Train loss: 4.013 Train rocauc: 0.501 Train Adv: 0.000 Valid loss: 7.064  Valid rocauc: 0.489         Test loss: 7.013  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:21<00:00,  1.56s/it]\n",
      "10it [00:09,  1.06it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: LR: 0.00025, Train loss: 4.014 Train rocauc: 0.497 Train Adv: 0.000 Valid loss: 7.064  Valid rocauc: 0.489         Test loss: 7.012  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:20<00:00,  1.54s/it]\n",
      "10it [00:09,  1.07it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: LR: 0.00017, Train loss: 4.016 Train rocauc: 0.506 Train Adv: 0.000 Valid loss: 7.063  Valid rocauc: 0.489         Test loss: 7.012  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:22<00:00,  1.57s/it]\n",
      "10it [00:09,  1.05it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: LR: 0.00009, Train loss: 4.015 Train rocauc: 0.499 Train Adv: 0.000 Valid loss: 7.063  Valid rocauc: 0.489         Test loss: 7.012  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:21<00:00,  1.55s/it]\n",
      "10it [00:09,  1.06it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: LR: 0.00002, Train loss: 4.015 Train rocauc: 0.501 Train Adv: 0.000 Valid loss: 7.063  Valid rocauc: 0.489         Test loss: 7.011  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:24<00:00,  1.59s/it]\n",
      "10it [00:09,  1.07it/s]\n",
      "  0%|          | 0/91 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: LR: 0.00000, Train loss: 4.012 Train rocauc: 0.508 Train Adv: 0.000 Valid loss: 7.063  Valid rocauc: 0.489         Test loss: 7.011  Test rocauc: 0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [02:20<00:00,  1.55s/it]\n",
      "10it [00:09,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_adv  = []\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    all_idx = torch.randperm(len(dataset))\n",
    "    for batch_idx in tqdm(range(len(all_idx) // args.bsz)):\n",
    "        batch = all_idx[batch_idx * args.bsz : (batch_idx + 1) * args.bsz]\n",
    "        train_msk = train_mask[batch]    \n",
    "        data = Batch.from_data_list(dataset[batch])\n",
    "        data.to(args.device)\n",
    "        \n",
    "        strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "#         strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "        out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        with torch.no_grad():\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            adv_out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "        # loss = criterion(out[train_msk], data.y[train_msk].float())\n",
    "        loss = 0.0\n",
    "        numel = 0\n",
    "        for idx, orig_data in enumerate(orig_dataset[batch]):\n",
    "            if not train_msk[idx]:\n",
    "                continue\n",
    "            orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "            normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "            pred_adj = torch.mm(normalized, normalized.t()) / args.temp\n",
    "            loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "            numel += pred_adj.numel()\n",
    "        loss = loss / numel\n",
    "        \n",
    "        adv_loss = entropy_loss(turn_prob(out[train_msk]).log(), turn_prob(adv_out[train_msk]))\n",
    "        adv_loss = adv_loss * 0\n",
    "        (loss + 0.5 * adv_loss).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += [loss.item()]\n",
    "        train_adv  += [adv_loss.item()]\n",
    "        \n",
    "        y_true += [data.y]\n",
    "        y_scores += [out]\n",
    "\n",
    "    input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "    train_metric = evaluator.eval(input_dict)[args.metric]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for num_iters, (data, orig_data_batch) in enumerate(tqdm(zip(valid_loader, orig_valid_loader))):\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            # loss = criterion(out, data.y.float())\n",
    "            loss = 0.0\n",
    "            numel = 0\n",
    "            for idx, orig_data in enumerate(orig_data_batch.to_data_list()):\n",
    "                orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "                normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "                pred_adj = torch.mm(normalized, normalized.t()) / args.temp\n",
    "                loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "                numel += pred_adj.numel()\n",
    "            loss = loss / numel\n",
    "            \n",
    "            valid_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        valid_metric = evaluator.eval(input_dict)[args.metric]\n",
    "        \n",
    "        test_loss = []\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        for (data, orig_data_batch) in zip(test_loader, orig_test_loader):\n",
    "            data.to(args.device)\n",
    "            strats = {'ea': data.edge_attr, \\\n",
    "                  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "            # strats = {'ea': data.edge_attr, 'rw': data.rw_edge_attr}\n",
    "            out, reps = model(data.x, data.batch, data.edge_index, strats)\n",
    "        \n",
    "            # loss = criterion(out, data.y.float())\n",
    "            loss = 0.0\n",
    "            numel = 0\n",
    "            for idx, orig_data in enumerate(orig_data_batch.to_data_list()):\n",
    "                orig_adj = to_dense_adj(edge_index=to_undirected(orig_data.edge_index), max_num_nodes=orig_data.x.size(0))[0].float().to(args.device)\n",
    "                normalized = F.normalize(reps[data.batch == idx][:-1])\n",
    "                pred_adj = torch.mm(normalized, normalized.t()) / args.temp\n",
    "                loss += criterion(pred_adj, orig_adj + torch.eye(orig_adj.size(0), device=orig_adj.device))\n",
    "                numel += pred_adj.numel()\n",
    "            loss = loss / numel\n",
    "        \n",
    "            test_loss += [loss.item()]\n",
    "\n",
    "            y_true += [data.y]\n",
    "            y_scores += [out]\n",
    "\n",
    "        input_dict = {\"y_true\": torch.cat(y_true), \"y_pred\": torch.cat(y_scores)}\n",
    "        test_metric = evaluator.eval(input_dict)[args.metric]\n",
    "\n",
    "    print('Epoch %d: LR: %.5f, Train loss: %.3f Train %s: %.3f Train Adv: %.3f Valid loss: %.3f  Valid %s: %.3f \\\n",
    "        Test loss: %.3f  Test %s: %.3f' \\\n",
    "          % (epoch + 1, optimizer.param_groups[0]['lr'], np.average(train_loss), args.metric, train_metric, \\\n",
    "             np.average(train_adv), np.average(valid_loss), args.metric, valid_metric, \\\n",
    "             np.average(test_loss), args.metric, test_metric))\n",
    "    stats += [[epoch, np.average(train_loss), train_metric, np.average(valid_loss), valid_metric, np.average(test_loss), test_metric]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = ['epoch', 'train_loss', 'train_metric', 'valid_loss', 'valid_metric', 'test_loss', 'test_metric']\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "stats_np = np.array(stats)\n",
    "best_valid = stats_np[stats_np[:50, 4].argmax()]\n",
    "print(best_valid)\n",
    "for i in range(1, stats_np.shape[-1]):\n",
    "    ax = fig.add_subplot(2, 3, i)\n",
    "    ax.plot(stats_np[:, i], label=labels[i])\n",
    "    ax.scatter(x=best_valid[0], y=best_valid[i], color='red')\n",
    "    ax.annotate(best_valid[i].round(3), xy=(best_valid[0]+5, best_valid[i]), color='red')\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.utils import degree\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "\n",
    "def generateSequence(startIndex, transitionMatrix, path_length, alpha):\n",
    "    result = [startIndex]\n",
    "    current = startIndex\n",
    "\n",
    "    for i in range(0, path_length):\n",
    "        if random.random() < alpha:\n",
    "            nextIndex = startIndex\n",
    "        else:\n",
    "            probs = transitionMatrix[current]\n",
    "            assert np.sum(probs) != 0, print(probs)\n",
    "            nextIndex = np.random.choice(len(probs), 1, p=probs)[0]\n",
    "\n",
    "        result.append(nextIndex)\n",
    "        current = nextIndex\n",
    "\n",
    "    return result\n",
    "\n",
    "def weighted_random_walk(data, transitionMatrix, path_length, alpha, degree_weighted_start=True, num_samples=3):\n",
    "    if degree_weighted_start:\n",
    "        # Exclude degree 1 nodes, soft max over remaining degrees\n",
    "        p = degree(data.edge_index[0])\n",
    "        p[p == 1] = 0\n",
    "    else:\n",
    "        p = torch.ones(data.num_nodes)    \n",
    "    m = Multinomial(num_samples, probs=p.exp()-1)\n",
    "    start_node = m.sample().long().tolist()\n",
    "    start = torch.Tensor(sum([[i] * start_node[i] for i in range(len(start_node))], [])).long()\n",
    "    \n",
    "    sentenceList = []\n",
    "    nodes = list(range(data.num_nodes))\n",
    "    \n",
    "    for j in range(0, num_samples):\n",
    "        indexList = generateSequence(start[j].item(), transitionMatrix, path_length, alpha)\n",
    "        sentence = [nodes[tmp] for tmp in indexList]\n",
    "        sentence = torch.LongTensor(sentence).unique()\n",
    "        sentenceList.append(sentence)\n",
    "\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "\n",
    "model.eval()\n",
    "threshold = 0.1\n",
    "\n",
    "for idx in range(20, 41):\n",
    "    orig_data = orig_dataset[idx]\n",
    "\n",
    "    data = dataset[idx]\n",
    "    data.to(args.device)\n",
    "    strats = {'ea': data.edge_attr,  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "    out, _ = model(data.x, 0, data.edge_index, strats)\n",
    "    \n",
    "    imgs = []\n",
    "    for layer_idx, gc in enumerate(model.gcs):\n",
    "\n",
    "        adj = to_dense_adj(edge_index=data.edge_index, edge_attr=gc.att)[0]\n",
    "        adj_mean = adj.mean(dim=-1).detach().cpu()\n",
    "        adj_mean = adj_mean[:-1, :-1] # remove virtual node\n",
    "        # only include edges that were in the original graph\n",
    "        orig_adj = to_dense_adj(edge_index=orig_data.edge_index)[0].bool()\n",
    "        adj_mean[~orig_adj] = 0\n",
    "\n",
    "#         adj_mean_sorted = adj_mean.flatten().sort()[0]\n",
    "#         adj_mean_sorted = adj_mean_sorted[adj_mean_sorted != 0]\n",
    "#         adj_mean_threshold = adj_mean_sorted[int(threshold * len(adj_mean_sorted))]\n",
    "#         adj_mean[adj_mean < adj_mean_threshold] = 0\n",
    "#         adj_mean[adj_mean >= adj_mean_threshold] = 1\n",
    "        subsetList = weighted_random_walk(orig_data, adj_mean.cpu().numpy(), 10, 0)\n",
    "        edgeIndexList = []\n",
    "        for s in subsetList:\n",
    "            edgeIndexList.append(torch.LongTensor(s).repeat_interleave(2)[1:-1].reshape(-1, 2))\n",
    "        mean_edge_index = torch.cat(edgeIndexList)\n",
    "    \n",
    "        # mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "        mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "\n",
    "        mol = pyg_to_mol(mean_data)  \n",
    "        svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "        imgs += [svg]\n",
    "\n",
    "    mol = pyg_to_mol(orig_data)\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from draw_mols_demo import pyg_to_mol, mol_to_svg, HorizontalDisplay\n",
    "\n",
    "model.eval()\n",
    "threshold = 0.0\n",
    "orig_dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
    "\n",
    "for idx in range(20, 41):\n",
    "    orig_data = orig_dataset[idx]\n",
    "\n",
    "    data = dataset[idx]\n",
    "    data.to(args.device)\n",
    "    strats = {'ea': data.edge_attr,  **{('rw_' + str(k)): data['rw_edge_attr_' + str(k)] for k in range(1, args.k_hop_neighbors + 1)}}\n",
    "    out, reps = model(data.x, 0, data.edge_index, strats)\n",
    "    \n",
    "    imgs = []\n",
    "    adj_mean = torch.mm(reps[:-1], reps[:-1].t()).detach().cpu()\n",
    "    # only include edges that were in the original graph\n",
    "    orig_adj = to_dense_adj(edge_index=orig_data.edge_index)[0].bool()\n",
    "    adj_mean[~orig_adj] = 0\n",
    "\n",
    "#     adj_mean_sorted = adj_mean.flatten().sort()[0]\n",
    "#     adj_mean_sorted = adj_mean_sorted[adj_mean_sorted != 0]\n",
    "#     adj_mean_threshold = adj_mean_sorted[int(threshold * len(adj_mean_sorted))]\n",
    "#     adj_mean[adj_mean < adj_mean_threshold] = 0\n",
    "#     adj_mean[adj_mean >= adj_mean_threshold] = 1\n",
    "    subsetList = weighted_random_walk(orig_data, adj_mean.cpu().numpy(), 10, 0)\n",
    "    edgeIndexList = []\n",
    "    for s in subsetList:\n",
    "        edgeIndexList.append(torch.LongTensor(s).repeat_interleave(2)[1:-1].reshape(-1, 2))\n",
    "    mean_edge_index = torch.cat(edgeIndexList)\n",
    "\n",
    "    mean_edge_index = dense_to_sparse(adj_mean.long())[0]\n",
    "    mean_data = Data(x=data.x, edge_index=mean_edge_index)\n",
    "\n",
    "    mol = pyg_to_mol(mean_data)  \n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "\n",
    "    mol = pyg_to_mol(orig_data)\n",
    "    svg = mol_to_svg(mol, molSize=(150, 150))\n",
    "    imgs += [svg]\n",
    "    row = HorizontalDisplay(*imgs)\n",
    "    display(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
